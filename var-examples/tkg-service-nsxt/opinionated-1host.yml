# SOFTWARE_DIR must contain all required software
vc_iso: "{{ lookup('env', 'SOFTWARE_DIR') }}/VMware-VCSA-all-7.0.2-17694817.iso"
esxi_ova: "{{ lookup('env', 'SOFTWARE_DIR') }}/Nested_ESXi7.0u2_Appliance_Template_v1.ova"
nsxt_ova: "{{ lookup('env', 'SOFTWARE_DIR') }}/nsx-unified-appliance-3.1.1.0.0.17483186.ova"

environment_tag: "tkg-nsxt" # Used to prepend object names in hosting vCenter
dns_server: "192.168.0.1"
dns_domain: "home.local"
ntp_server_ip: "192.168.0.1" # Must be set to an IP address!
disk_mode: thin # How all disks should be deployed
nested_host_password: "{{ opinionated.master_password }}"

hosting_vcenter: # This is the vCenter which will be the target for nested vCenters and ESXi hosts
  ip: "192.168.0.113"
  username: "{{ lookup('env', 'PARENT_VCENTER_USERNAME') }}"
  password: "{{ lookup('env', 'PARENT_VCENTER_PASSWORD') }}"
  datacenter: "Home" # Target for all VM deployment

# This section describes what will be created
opinionated:
  master_password: "VMware1!"
  nested_hosts:
    cpu_cores: 12 # CPU count per nested host
    ram_in_gb: 96 # memory per nested host
    local_disks:
      - size_gb: 500
        datastore_prefix: "datastore"
  hosting_cluster: Physical
  hosting_datastore: NVME
  hosting_network:
    base:
      port_group: Nest
      cidr: "192.168.0.0/22"
      gateway: "192.168.0.1"
      # A NSX-T deployment requires 4 IPs, plus 1 per esxi host. They MUST be contiguous.
      starting_addr: "192.168.0.160"
    # nsxt tep pool will not be routed, but should not clash with routeable ranges
    nsxt_tep:
      port_group: TEP
      vlan_id: 0
      cidr: "172.31.255.248/29" # Should be at least a 29 which supports up to 5 hosts and 1 edge
  tkg_service:
    # This network must be minimum of a /25
    # 1/8 of the network will be used for the supervisor
    # 3/8 of the network will be used for egress IP range
    # 1/2 of the network will be used for ingress IP range
    routeable_super_net: 172.20.0.0/22
    # This network is private and should not overlap with any routeable networks
    internal_pod_network: 172.20.4.0/22
    # This network is private and should not overlap with any routeable networks 
    internal_kubernetes_services_network: 172.20.8.0/22

#####################################################################
### No need to edit below this line for an opinionated deployment ###
#####################################################################

nested_vcenter: # the vCenter appliance that will be deployed
  ip: "{{ opinionated.hosting_network.base.starting_addr }}" # vCenter ip address 
  mask: "{{ opinionated.hosting_network.base.cidr.split('/')[1] }}"
  gw: "{{ opinionated.hosting_network.base.gateway }}"
  host_name: "{{ opinionated.hosting_network.base.starting_addr }}" # FQDN if there is working DNS server, otherwise put the ip as a name
  username: "administrator@vsphere.local" 
  password: "{{ opinionated.master_password }}"
  datacenter: "Lab" # DC to create after deployment
  # Below are properties of parent cluster
  hosting_network: "{{ opinionated.hosting_network.base.port_group }}" # Parent port group where the vCenter VM will be deployed
  hosting_cluster: "{{ opinionated.hosting_cluster }}" # Parent cluster where the vCenter VM will be deployed
  hosting_datastore: "{{ opinionated.hosting_datastore }}" # Parent datastore where the vCenter VM will be deployed
  
nested_clusters: # You can add clusters in this section by duplicating the existing cluster
  compute: # This will be the name of the cluster in the nested  vCenter. Below are the minimum settings.
    enable_drs: True
    enable_ha: True
    # Below are properties of the hosting cluster
    hosting_cluster: "{{ opinionated.hosting_cluster }}" # the cluster where physical ESXi is connected to. The nested VMs will be deployed here
    hosting_datastore: "{{ opinionated.hosting_datastore }}" # Datastore target for nested ESXi VMs
    # Settings below are assigned to each host in the cluster
    vswitch0_vm_port_group_name: vm-network
    vswitch0_vm_port_group_vlan: "0"
    cpu_cores: "{{ opinionated.nested_hosts.cpu_cores }}" # CPU count
    ram_in_gb: "{{ opinionated.nested_hosts.ram_in_gb }}" # memory
    # In order list of disks to assign to the nested host. All will be marked as SSD.
    # Datastore names will be automatically be pre-pended with the hostname. E.g esx1
    # If the datastore_prefix property is removed the disk will not be set as a datastore
    # To leave the default OVA disks in place, delete this section.
    nested_hosts_disks: "{{ opinionated.nested_hosts.local_disks | default(omit) }}"
    # Added in vmnic order, these port groups must exist on the physical host
    # Must specify at least 2 port groups, up to a maximum of 10
    vmnic_physical_portgroup_assignment: 
      - name: "{{ opinionated.hosting_network.base.port_group }}"
      - name: "{{ opinionated.hosting_network.nsxt_tep.port_group }}"
      - name: "{{ opinionated.hosting_network.nsxt_tep.port_group }}"

nested_hosts: 
  - name: "esx1"
    ip: "{{ opinionated.hosting_network.base.starting_addr | ipmath(4) }}"
    mask: "{{ opinionated.hosting_network.base.cidr | ansible.netcommon.ipaddr('netmask') }}"
    gw: "{{ opinionated.hosting_network.base.gateway }}"
    nested_cluster: compute

distributed_switches: # (optional) - section can be removed to not create any distributed switches
  - vds_name: nsxt-vds
    mtu: 1600
    vds_version: 7.0.0 # Should be 7.0.0, 6.7.0 
    clusters: # distributed switch will be attached to all hosts in the clusters listed
      - compute
    uplink_quantity: 1
    vmnics:
      - vmnic1
  - vds_name: nsxt-edge-tep
    mtu: 1600
    vds_version: 7.0.0 # Should be 7.0.0, 6.7.0 
    clusters: # distributed switch will be attached to all hosts in the clusters listed
      - compute
    uplink_quantity: 1
    vmnics:
      - vmnic2
    distributed_port_groups: # (optional) - section can be removed to not create any distributed port groups
      - port_group_name: tep-pg
        vlan_id: "{{ opinionated.hosting_network.nsxt_tep.vlan_id |default(0) }}" 

tspbm: # Tag-based Storage Policy Based Management
  tag_categories:
    - category_name: tkgs-storage-category
      description: "TKGS tag category"
      tags:
        - tag_name: tkgs-storage-tag
          description: "Tag for datastores used by TKGS"
  datastore_tags:
    - datastore_name: "{{ opinionated.nested_hosts.local_disks[0].datastore_prefix }}-esx1"
      tag_names:
        - tkgs-storage-tag
  vm_storage_policies:
    - storage_policy_name: tkgs-storage-policy
      description: "TKGS storage performance policy"
      tag_name: tkgs-storage-tag
      tag_category: tkgs-storage-category

tkg_service:
  cluster_ip_services_cidr: "{{ opinionated.tkg_service.internal_kubernetes_services_network }}" # This will not be routed and is private within each cluster
  content_library_datastore: "{{ opinionated.nested_hosts.local_disks[0].datastore_prefix }}-esx1"
  content_library_name: tkgs-library
  content_library_url: "http://wp-content.vmware.com/v2/latest/lib.json"
  default_content_library: tkgs-library
  dns_server_list: ["{{ dns_server }}"]
  ephemeral_storage_policy: "{{ tspbm.vm_storage_policies[0].storage_policy_name }}"
  fluentbit_enabled: false
  image_storage_policy: "{{ tspbm.vm_storage_policies[0].storage_policy_name }}"
  ntp_server_list: ["{{ ntp_server_ip }}"]
  management_dns_servers: ["{{ dns_server }}"]
  management_port_group: supervisor-seg
  management_gateway: "{{ opinionated.tkg_service.routeable_super_net | ipmath(1) }}"
  management_netmask: >-
    {{ opinionated.tkg_service.routeable_super_net | 
    ansible.netcommon.ipsubnet((opinionated.tkg_service.routeable_super_net.split('/')[1] |int)+3, 0) |
    ansible.netcommon.ipaddr('netmask') }}
  management_starting_address: "{{ opinionated.tkg_service.routeable_super_net | ipmath(2) }}"
  master_storage_policy: "{{ tspbm.vm_storage_policies[0].storage_policy_name }}"
  network_provider: NSXT_CONTAINER_PLUGIN
  supervisor_size: tiny
  vsphere_cluster: compute
  workload_dns_servers: ["{{ dns_server }}"]

  nsxt:
    cluster_distributed_switch: "{{ distributed_switches[0].vds_name }}"
    egress_cidrs:
      - "{{ opinionated.tkg_service.routeable_super_net | ansible.netcommon.ipsubnet((opinionated.tkg_service.routeable_super_net.split('/')[1] |int)+3, 1) }}"
      - "{{ opinionated.tkg_service.routeable_super_net | ansible.netcommon.ipsubnet((opinionated.tkg_service.routeable_super_net.split('/')[1] |int)+2, 1) }}"
    ingress_cidrs:
      - "{{ opinionated.tkg_service.routeable_super_net | ansible.netcommon.ipsubnet((opinionated.tkg_service.routeable_super_net.split('/')[1] |int)+1, 1) }}"
    nsx_edge_cluster: "{{ nsxt.edge_clusters[0].edge_cluster_name}}"
    pod_cidrs: "{{ opinionated.tkg_service.internal_pod_network }}"
    # This is used by the task which checks if the supervisor network is online
    t0_uplink_ip: "{{ opinionated.hosting_network.base.starting_addr | ipmath(3) }}"
  

nsxt: # (optional) - section can be removed to not create any nsxt objects
  manager:
    hostname: "{{ opinionated.hosting_network.base.starting_addr | ipmath(1) }}"
    ip: "{{ opinionated.hosting_network.base.starting_addr | ipmath(1) }}"
    netmask: "{{ opinionated.hosting_network.base.cidr | ansible.netcommon.ipaddr('netmask') }}"
    gateway: "{{ opinionated.hosting_network.base.gateway }}"
    username: admin # this cannot be changed
    password: "{{ opinionated.master_password }}{{ opinionated.master_password }}"
    hosting_vcenter_ip: "{{ hosting_vcenter.ip }}"
    hosting_vcenter_username: "{{ hosting_vcenter.username }}"
    hosting_vcenter_password: "{{ hosting_vcenter.password }}"
    hosting_datacenter: "{{ hosting_vcenter.datacenter }}"
    hosting_datastore: "{{ opinionated.hosting_datastore }}"
    hosting_network: "{{ opinionated.hosting_network.base.port_group }}"
    hosting_cluster: "{{ opinionated.hosting_cluster }}"
    license_key: "{{ lookup('env', 'NSXT_LICENSE_KEY') }}"
  
  # If the section below is defined, the playbook will wait for the IP to become pingable
  # For TKG service deployments this is the default gateway of the supervisor network
  routing_test: 
    ip_to_ping: "{{ opinionated.tkg_service.routeable_super_net | ipmath(1) }}"
    # The playbook will present a message using the params below
    # A static route must be made to the router uplink for nsxt_supernet
    router_uplink: "{{ opinionated.hosting_network.base.starting_addr | ipmath(3) }}"
    nsxt_supernet: "{{ opinionated.tkg_service.routeable_super_net }}"

  ip_pools:
    - display_name: tep-pool # This is a non-routable range which is used for the overlay tunnels.
      subnets:
        - allocation_ranges:
            - start: "{{ opinionated.hosting_network.nsxt_tep.cidr | ipmath(1) }}"
              end: "{{ opinionated.hosting_network.nsxt_tep.cidr | ansible.netcommon.ipaddr('-2') | ipaddr('address') }}"
          cidr: "{{ opinionated.hosting_network.nsxt_tep.cidr }}"
  
  uplink_profiles:
    - display_name: host-tep-profile
      teaming:
        active_list:
          - uplink_name: "uplink-1"
            uplink_type: PNIC
        policy: FAILOVER_ORDER
      transport_vlan: 0  
    - display_name: edge-tep-profile
      mtu: 9000
      teaming:
        active_list:
          - uplink_name: "uplink-1"
            uplink_type: PNIC
        policy: FAILOVER_ORDER
      transport_vlan: 0  
    - display_name: edge-uplink-profile
      mtu: 1500
      teaming:
        active_list:
          - uplink_name: "uplink-1"
            uplink_type: PNIC
        policy: FAILOVER_ORDER
      transport_vlan: 0

  transport_zones:
    - display_name: tz-overlay
      transport_type: OVERLAY
      # host_switch_name: "{{ distributed_switches[0].vds_name }}"
      nested_nsx: True # Set this to True if you use NSX-T for your physical host networking
      description: "Overlay Transport Zone"
    - display_name: tz-vlan
      transport_type: VLAN
      # host_switch_name: sw_vlan
      description: "Uplink Transport Zone"

  transport_node_profiles:
    - display_name: tnp1
      host_switches:
        - host_switch_profiles:
            - name: host-tep-profile
              type: UplinkHostSwitchProfile
          host_switch_name: "{{ distributed_switches[0].vds_name }}"
          host_switch_type: VDS
          host_switch_mode: STANDARD
          ip_assignment_spec:
            resource_type: StaticIpPoolSpec
            ip_pool_name: "tep-pool"
          transport_zone_endpoints:
            - transport_zone_name: "tz-overlay"
          uplinks:
            - uplink_name: "uplink-1"
              vds_uplink_name: "Uplink 1"
      description: "Cluster node profile"

  cluster_attach:
    - display_name: "tnc1"
      description: "Transport Node Collections 1"
      compute_manager_name: "vCenter"
      cluster_name: "compute"
      transport_node_profile_name: "tnp1"

  edge_nodes:
    - display_name: edge-node-1
      size: MEDIUM
      mgmt_ip_address: "{{ opinionated.hosting_network.base.starting_addr | ipmath(2) }}"
      mgmt_prefix_length: "{{ opinionated.hosting_network.base.cidr.split('/')[1] }}"
      mgmt_default_gateway: "{{ opinionated.hosting_network.base.gateway }}"
      network_management_name: vm-network
      network_uplink_name: vm-network
      network_tep_name: tep-pg
      datastore_name: datastore-esx1
      cluster_name: compute
      host_switches:
        tep:
          uplink_profile_name: edge-tep-profile
          ip_assignment_spec:
            resource_type: StaticIpPoolSpec
            ip_pool_name: tep-pool
          transport_zone_endpoints:
            - transport_zone_name: "tz-overlay"
        uplink:
          host_switch_name: "sw_vlan"
          uplink_profile_name: edge-uplink-profile
          transport_zone_endpoints:
            - transport_zone_name: "tz-vlan"
      transport_zone_endpoints:
        - transport_zone_name: "tz-overlay-2"
        - transport_zone_name: "tz-vlan"

  edge_clusters:
  - edge_cluster_name: edge-cluster-1
    edge_cluster_members:
      - transport_node_name: edge-node-1

  uplink_segments:
    - display_name: t0-uplink
      vlan_ids: [0]
      transport_zone_display_name: tz-vlan

  # For full spec see - https://github.com/laidbackware/ansible-for-nsxt/blob/vmware-lab-builder/library/nsxt_policy_tier0.py
  tier_0:
    display_name: "tkgs-t0"
    ha_mode: ACTIVE_STANDBY
    uplink_ip: "{{ opinionated.hosting_network.base.starting_addr | ipmath(3) }}"
    disable_firewall: True
    static_routes: 
      - state: present
        display_name: default-route
        network: "0.0.0.0/0"
        next_hops:
          - ip_address: "{{ opinionated.hosting_network.base.gateway }}"
    locale_services:
      - state: present
        display_name: "tkgs-t0-ls"
        edge_cluster_info:
          edge_cluster_display_name: edge-cluster-1
        interfaces:
          - display_name: "test-t0-t0ls-iface"
            state: present
            subnets:
              - ip_addresses: ["{{ opinionated.hosting_network.base.starting_addr | ipmath(3) }}"]
                prefix_len: "{{ opinionated.hosting_network.base.cidr.split('/')[1] | int }}"
            segment_id: t0-uplink
            edge_node_info:
              edge_cluster_display_name: edge-cluster-1
              edge_node_display_name: edge-node-1
            mtu: 1500

  overlay_segments:
    - display_name: supervisor-seg
      transport_zone_display_name: tz-overlay
      tier1_display_name: supervisor-t1
      subnets:
        - gateway_address: "{{ opinionated.tkg_service.routeable_super_net | ipmath(1) }}/{{ opinionated.tkg_service.routeable_super_net.split('/')[1] |int +3 }}"

  tier_1_gateways: 
    - display_name: supervisor-t1
      route_advertisement_types:
        - "TIER1_CONNECTED"
      tier0_display_name: tkgs-t0